{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapté de https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "\n",
    "# Qu'est-ce que PyTorch?\n",
    "\n",
    "-  Une librairie pouvant remplacer **NumPy** et permettant une **accélération GPU**\n",
    "-  Une plateforme pour la **recherche en Deep Learning** offrant un maximum de **flexibilité et de rapidité**\n",
    "\n",
    "## Tenseurs (`Tensor`)\n",
    "\n",
    "Les tenseurs sont similaires aux `ndarray` de NumPy, et peuvent être utilisés sur GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construisons une matrice 5x3 non-initialisée:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construisons une matrice 5x3 de nombre aléatoires (entre 0 et 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construisons une matrice remplie de zéros, et de type `long`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construisons une matrice à partir de données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 0], [0, 1]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Les fonctions `torch.*_like(x)` fonctionnent de la même façon qu'avec NumPy (e.g. `zeros_like(x)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons les dimensions du tenseur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.size())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: `torch.Size` est une sous-classe de `tuple`.\n",
    "\n",
    "Vous pouvez aussi obtenir la taille d'une seule dimension d'un coup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.size(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opérations\n",
    "\n",
    "Les opérations sur les tenseurs peuvent être faites avec différentes syntaxes. Regardons l'exemple de l'addition.\n",
    "\n",
    "**Addition: syntaxe 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.ones_like(x)\n",
    "\n",
    "somme = x + y\n",
    "print(somme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Addition: syntaxe 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somme = torch.add(x, y)\n",
    "print(somme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Addition: fournir un tenseur de destination**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "somme = torch.empty(5, 3)\n",
    "torch.add(x, y, out=somme)\n",
    "print(somme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Addition: \"in-place\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.add_(x)  # équivalent à torch.add(x, y, out=y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: les méthodes opérant \"in-place\" ont le suffixe `_`. Par exemple, `x.t_()` transpose `x`, alors que `x.t()` retourne un nouveau tenseur contenant `x` transposé.\n",
    "\n",
    "**Indiçage**\n",
    "\n",
    "Vous pouvez utiliser la même syntaxe qu'avec NumPy, même pour les cas complexes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[:, 1])\n",
    "print(x[:, np.newaxis, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.reshape` est équivalent à `numpy.reshape`. Cependant, `torch.view` est plus souvent utilisée (mais ne fonctionne que lorsqu'une les valeurs n'ont pas à être copiées):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 4)\n",
    "print(x.size())\n",
    "\n",
    "y = x.view(16)\n",
    "print(y.size())\n",
    "\n",
    "z = x.view(-1, 8)  # -1 permet d'inférer automatiquement les autres dimensions\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous avez un tenseur contenant un seul élément, utilisez `.item()` pour obtenir la valeur sous forme de nombre Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4).mean()\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**À voir plus tard:** 100+ opérations, includant transposée, indiçage, _slicing_, algèbre linéaire, nombres aléatoires, etc. : https://pytorch.org/docs/torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interopérabilité avec NumPy\n",
    "\n",
    "La conversion d'un tenseur PyTorch en un NumPy array, et vice versa, est simple comme bonjour.\n",
    "\n",
    "Le tenseur et le NumPy array utiliseront le même espace mémoire (si le tenseur n'est pas sur GPU); modifier l'un modifie l'autre du même coup.\n",
    "\n",
    "## Conversion PyTorch --> NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones(5)\n",
    "print(type(a))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.numpy()\n",
    "print(type(b))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifions le tenseur et observons le changement dans le NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion NumPy --> PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Placer et utiliser des tenseurs sur GPU\n",
    "\n",
    "Les tenseurs peuvent être placés sur un appareil (`torch.device`) avec la méthode `.to`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.device_count() > 0, \"On va avoir besoin d'un GPU\"\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "x = torch.rand(2, 2, device=device)    # créer un tenseur directement sur GPU\n",
    "y = torch.ones_like(x)\n",
    "x = x.to(device)                       # placer un tenseur existant sur GPU\n",
    "z = x + y\n",
    "print(z)\n",
    "print(z.to(\"cpu\", torch.double))       # ``.to`` peut aussi changer le type du même coup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
